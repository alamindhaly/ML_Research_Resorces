{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alamindhaly/ML_Research_Resorces/blob/main/Anti_Money_Laundering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l_xc4AmbJuvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4088ea39-931a-415f-f04e-e5caaec96d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install pandas numpy scikit-learn torch torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4KQGmlrKWQu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import json\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from torch_geometric.data import Data as GeoData\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPM1SZSSKvvn",
        "outputId": "3eea40e0-159b-41f3-df72-5a2df2f45669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Basic training settings\n",
        "SEQ_LEN = 10          # length of transaction sequence per account\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 5            # keep small for quick runs; increase later\n",
        "LR = 1e-3\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "np.random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V0eG7aVK1rs",
        "outputId": "785a44bd-1678-4f37-ae1d-1750b7cf3e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWL_vV-1L6nd",
        "outputId": "9c2669be-757b-4e77-bf6d-728dd024127e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CSV: /content/drive/MyDrive/Research Folder (Al-Amin Dhaly)/SAML-D.csv\n"
          ]
        }
      ],
      "source": [
        "CSV_PATH = \"/content/drive/MyDrive/Research Folder (Al-Amin Dhaly)/SAML-D.csv\"\n",
        "print(\"Using CSV:\", CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbyQRYDcMmws"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO73I18AOI9f",
        "outputId": "5765a427-0f81-4593-ea0f-6ba96016a204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding transaction features...\n",
            "Transaction feature dimension: 71\n",
            "Total transactions: 1048575\n"
          ]
        }
      ],
      "source": [
        "# Combine Date and Time into a single datetime column\n",
        "df[\"Datetime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"], dayfirst=True)\n",
        "df = df.sort_values(\"Datetime\").reset_index(drop=True)\n",
        "\n",
        "# Convert datetime to numeric seconds (for time-based feature)\n",
        "df[\"Timestamp\"] = (df[\"Datetime\"] - df[\"Datetime\"].min()).dt.total_seconds()\n",
        "\n",
        "# Map account IDs to integer node IDs for graph\n",
        "accounts = pd.Index(df[\"Sender_account\"]).append(pd.Index(df[\"Receiver_account\"])).unique()\n",
        "acc_to_idx = {acc: i for i, acc in enumerate(accounts)}\n",
        "df[\"sender_idx\"] = df[\"Sender_account\"].map(acc_to_idx)\n",
        "df[\"receiver_idx\"] = df[\"Receiver_account\"].map(acc_to_idx)\n",
        "\n",
        "# Binary label: 1 = laundering, 0 = normal\n",
        "df[\"label\"] = df[\"Is_laundering\"].astype(np.float32)\n",
        "\n",
        "# Choose feature columns\n",
        "cat_cols = [\n",
        "    \"Payment_currency\",\n",
        "    \"Received_currency\",\n",
        "    \"Sender_bank_location\",\n",
        "    \"Receiver_bank_location\",\n",
        "    \"Payment_type\",\n",
        "]\n",
        "num_cols = [\"Amount\", \"Timestamp\"]\n",
        "\n",
        "print(\"Encoding transaction features...\")\n",
        "# One-hot encode categorical features\n",
        "df_cat = pd.get_dummies(df[cat_cols], drop_first=False)\n",
        "txn_features_df = pd.concat([df[num_cols], df_cat], axis=1)\n",
        "\n",
        "# Scale transaction features\n",
        "scaler_txn = StandardScaler()\n",
        "txn_features = scaler_txn.fit_transform(txn_features_df.values).astype(np.float32)\n",
        "feature_dim = txn_features.shape[1]\n",
        "\n",
        "labels = df[\"label\"].values.astype(np.float32)\n",
        "sender_idx_arr = df[\"sender_idx\"].values.astype(np.int64)\n",
        "receiver_idx_arr = df[\"receiver_idx\"].values.astype(np.int64)\n",
        "\n",
        "print(\"Transaction feature dimension:\", feature_dim)\n",
        "print(\"Total transactions:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J2s-ILBO1bW",
        "outputId": "a94fd835-2c45-462a-b84b-f18fa25b6830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 325888\n",
            "Number of edges: 1048575\n"
          ]
        }
      ],
      "source": [
        "#Build graph data for the GAT branch\n",
        "\n",
        "num_nodes = len(accounts)\n",
        "\n",
        "# Simple node features: outgoing count/sum, incoming count/sum\n",
        "node_feats = np.zeros((num_nodes, 4), dtype=np.float32)\n",
        "\n",
        "out_stats = df.groupby(\"sender_idx\")[\"Amount\"].agg([\"count\", \"sum\"])\n",
        "in_stats = df.groupby(\"receiver_idx\")[\"Amount\"].agg([\"count\", \"sum\"])\n",
        "\n",
        "node_feats[out_stats.index, 0] = out_stats[\"count\"].values\n",
        "node_feats[out_stats.index, 1] = out_stats[\"sum\"].values\n",
        "node_feats[in_stats.index, 2] = in_stats[\"count\"].values\n",
        "node_feats[in_stats.index, 3] = in_stats[\"sum\"].values\n",
        "\n",
        "# Scale node features\n",
        "scaler_node = StandardScaler()\n",
        "node_feats = scaler_node.fit_transform(node_feats).astype(np.float32)\n",
        "\n",
        "# Edges: sender -> receiver\n",
        "edge_index = torch.tensor(\n",
        "    np.vstack([sender_idx_arr, receiver_idx_arr]),\n",
        "    dtype=torch.long,\n",
        ")\n",
        "\n",
        "node_x = torch.tensor(node_feats, dtype=torch.float32)\n",
        "\n",
        "graph_data = GeoData(x=node_x, edge_index=edge_index)\n",
        "\n",
        "print(\"Number of nodes:\", node_x.shape[0])\n",
        "print(\"Number of edges:\", edge_index.shape[1])\n",
        "\n",
        "# Move graph tensors to GPU/CPU device once\n",
        "node_x_gpu = graph_data.x.to(DEVICE)\n",
        "edge_index_gpu = graph_data.edge_index.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAEDogOZPCPG",
        "outputId": "74c68fe1-866e-4478-cbd5-fc82693e930c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporal sequence dataset...\n",
            "Total sequence samples: 619077\n",
            "Train samples: 495261\n",
            "Test samples : 123816\n"
          ]
        }
      ],
      "source": [
        "# Build sequence dataset for temporal branch\n",
        "\n",
        "class AMLSequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each sample:\n",
        "      - a sequence of SEQ_LEN past transactions for one sender\n",
        "      - sender index, receiver index of last transaction\n",
        "      - label of last transaction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, txn_features, seq_len=10, max_samples_per_account=200):\n",
        "        self.seq_len = seq_len\n",
        "        self.txn_features = txn_features\n",
        "        self.labels = df[\"label\"].values.astype(np.float32)\n",
        "        self.sender_idx = df[\"sender_idx\"].values.astype(np.int64)\n",
        "        self.receiver_idx = df[\"receiver_idx\"].values.astype(np.int64)\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "        # Group transactions by sender account (in time order)\n",
        "        grouped = df.sort_values(\"Datetime\").groupby(\"sender_idx\").indices\n",
        "        rng = np.random.default_rng(RANDOM_STATE)\n",
        "\n",
        "        for sender, idxs in grouped.items():\n",
        "            idxs = np.sort(idxs)\n",
        "            if len(idxs) < seq_len:\n",
        "                continue\n",
        "\n",
        "            windows = []\n",
        "            # Sliding window of length seq_len\n",
        "            for i in range(len(idxs) - seq_len + 1):\n",
        "                window = idxs[i : i + seq_len]\n",
        "                target_idx = idxs[i + seq_len - 1]\n",
        "                windows.append((window, target_idx))\n",
        "\n",
        "            # Limit huge accounts to a max number of samples\n",
        "            if len(windows) > max_samples_per_account:\n",
        "                chosen = rng.choice(len(windows), size=max_samples_per_account, replace=False)\n",
        "                windows = [windows[i] for i in chosen]\n",
        "\n",
        "            self.samples.extend(windows)\n",
        "\n",
        "        print(f\"Total sequence samples: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window, target_idx = self.samples[idx]\n",
        "        # Sequence of features\n",
        "        seq_x = self.txn_features[window]  # (seq_len, feature_dim)\n",
        "        # Last transaction info\n",
        "        y = self.labels[target_idx]\n",
        "        s = self.sender_idx[target_idx]\n",
        "        r = self.receiver_idx[target_idx]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(seq_x, dtype=torch.float32),\n",
        "            torch.tensor(s, dtype=torch.long),\n",
        "            torch.tensor(r, dtype=torch.long),\n",
        "            torch.tensor(y, dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "print(\"Creating temporal sequence dataset...\")\n",
        "full_dataset = AMLSequenceDataset(df, txn_features, seq_len=SEQ_LEN)\n",
        "\n",
        "# Train/test split on sequences\n",
        "train_size = int((1 - TEST_SIZE) * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(RANDOM_STATE),\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Test samples :\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG1_z9LbPTy2",
        "outputId": "efbbf0b5-055b-4682-c493-e233d6477433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blockchain initialized. Length: 1\n"
          ]
        }
      ],
      "source": [
        "#Simple blockchain simulation for AML alerts\n",
        "\n",
        "class SimpleBlockchainAML:\n",
        "    \"\"\"\n",
        "    Very small blockchain-style log for high-risk alerts.\n",
        "    Each block stores a hash of alert info + previous hash.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.chain = []\n",
        "        self._create_genesis_block()\n",
        "\n",
        "    def _create_genesis_block(self):\n",
        "        block = {\n",
        "            \"index\": 0,\n",
        "            \"prev_hash\": \"0\" * 64,\n",
        "            \"data\": \"Genesis Block\",\n",
        "        }\n",
        "        block[\"hash\"] = self._hash_block(block)\n",
        "        self.chain.append(block)\n",
        "\n",
        "    def _hash_block(self, block_data):\n",
        "        # Hash the block content to create a unique fingerprint\n",
        "        block_str = json.dumps(block_data, sort_keys=True).encode(\"utf-8\")\n",
        "        return hashlib.sha256(block_str).hexdigest()\n",
        "\n",
        "    def add_alert(self, tx_info, risk_score):\n",
        "        prev_block = self.chain[-1]\n",
        "        block = {\n",
        "            \"index\": len(self.chain),\n",
        "            \"prev_hash\": prev_block[\"hash\"],\n",
        "            \"data\": {\n",
        "                \"tx_info\": tx_info,\n",
        "                \"risk_score\": float(risk_score),\n",
        "            },\n",
        "        }\n",
        "        block[\"hash\"] = self._hash_block(block)\n",
        "        self.chain.append(block)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.chain)\n",
        "\n",
        "    def tail(self, n=5):\n",
        "        # Show last n blocks\n",
        "        return self.chain[-n:]\n",
        "\n",
        "\n",
        "blockchain = SimpleBlockchainAML()\n",
        "print(\"Blockchain initialized. Length:\", len(blockchain))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc9neH4SPcYN",
        "outputId": "963bd1a0-db78-4101-c2d2-bd7f04b3bdf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HybridAMLModel(\n",
            "  (input_proj): Linear(in_features=71, out_features=64, bias=True)\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
            "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pos_encoder): PositionalEncoding()\n",
            "  (gat1): GATConv(4, 32, heads=2)\n",
            "  (fusion): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define the hybrid Transformer + GAT model\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    #Adds position information to sequence embeddings.\n",
        "\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len]\n",
        "\n",
        "\n",
        "class HybridAMLModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Temporal branch: Transformer on transaction sequences.\n",
        "    Graph branch: GAT on account graph.\n",
        "    Outputs probability of money laundering.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        txn_input_dim,\n",
        "        node_input_dim,\n",
        "        d_model=64,\n",
        "        nhead=4,\n",
        "        num_layers=1,      # 1 layer to keep things simple\n",
        "        gat_hidden=32,     # smaller hidden size\n",
        "        fusion_hidden=64,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Project transaction features to model dimension\n",
        "        self.input_proj = nn.Linear(txn_input_dim, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # GAT layers (graph branch)\n",
        "        self.gat1 = GATConv(node_input_dim, gat_hidden, heads=2, concat=False)\n",
        "\n",
        "        # Fusion: combine temporal + sender + receiver vectors\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(d_model + gat_hidden * 2, fusion_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(fusion_hidden, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, seq_x, sender_idx, receiver_idx, node_x, edge_index):\n",
        "        # Temporal branch\n",
        "        h = self.input_proj(seq_x)\n",
        "        h = self.pos_encoder(h)\n",
        "        h = self.transformer_encoder(h)\n",
        "        temporal_emb = h[:, -1, :]  # representation of last time step\n",
        "\n",
        "        # Graph branch\n",
        "        g = F.elu(self.gat1(node_x, edge_index))\n",
        "\n",
        "        sender_emb = g[sender_idx]\n",
        "        receiver_emb = g[receiver_idx]\n",
        "        graph_emb = torch.cat([sender_emb, receiver_emb], dim=1)\n",
        "\n",
        "        # Fuse temporal + graph features\n",
        "        fused = torch.cat([temporal_emb, graph_emb], dim=1)\n",
        "        logits = self.fusion(fused).squeeze(1)  # raw scores\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = HybridAMLModel(\n",
        "    txn_input_dim=feature_dim,\n",
        "    node_input_dim=node_x.shape[1],\n",
        "    d_model=64,\n",
        "    nhead=4,\n",
        "    num_layers=1,\n",
        "    gat_hidden=32,\n",
        "    fusion_hidden=64,\n",
        ").to(DEVICE)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk-fWyUaPqpt"
      },
      "outputs": [],
      "source": [
        "#Training and evaluation functions\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for seq_x, s_idx, r_idx, y in train_loader:\n",
        "        seq_x = seq_x.to(DEVICE)\n",
        "        s_idx = s_idx.to(DEVICE)\n",
        "        r_idx = r_idx.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(seq_x, s_idx, r_idx, node_x_gpu, edge_index_gpu)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * seq_x.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch:02d} - Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "def evaluate(blockchain_obj=None, alert_threshold=0.9):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq_x, s_idx, r_idx, y in test_loader:\n",
        "            seq_x = seq_x.to(DEVICE)\n",
        "            s_idx = s_idx.to(DEVICE)\n",
        "            r_idx = r_idx.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "\n",
        "            logits = model(seq_x, s_idx, r_idx, node_x_gpu, edge_index_gpu)\n",
        "            probs = torch.sigmoid(logits)\n",
        "\n",
        "            # log high-risk predictions to blockchain\n",
        "            if blockchain_obj is not None:\n",
        "                probs_cpu = probs.cpu().numpy()\n",
        "                s_cpu = s_idx.cpu().numpy()\n",
        "                r_cpu = r_idx.cpu().numpy()\n",
        "                y_cpu = y.cpu().numpy()\n",
        "\n",
        "                for i in range(len(probs_cpu)):\n",
        "                    risk = probs_cpu[i]\n",
        "                    if risk >= alert_threshold:\n",
        "                        tx_info = {\n",
        "                            \"sender_node\": int(s_cpu[i]),\n",
        "                            \"receiver_node\": int(r_cpu[i]),\n",
        "                            \"true_label\": int(y_cpu[i]),\n",
        "                        }\n",
        "                        blockchain_obj.add_alert(tx_info, risk_score=risk)\n",
        "\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(all_labels)\n",
        "    y_prob = np.concatenate(all_probs)\n",
        "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob)\n",
        "    except ValueError:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    print(f\"Test Accuracy : {acc:.4f}\")\n",
        "    print(f\"Test Precision: {prec:.4f}\")\n",
        "    print(f\"Test Recall   : {rec:.4f}\")\n",
        "    print(f\"Test F1       : {f1:.4f}\")\n",
        "    print(f\"Test AUC      : {auc:.4f}\")\n",
        "\n",
        "    return acc, prec, rec, f1, auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZONs8JlTRidG",
        "outputId": "9e74dc89-8837-445a-a683-69a38dd28481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 1/5  (20.0% completed)\n",
            "Epoch 01 - Train Loss: 0.0060\n",
            "Evaluating on test set...\n",
            "Test Accuracy : 0.9996\n",
            "Test Precision: 0.0000\n",
            "Test Recall   : 0.0000\n",
            "Test F1       : 0.0000\n",
            "Test AUC      : 0.8834\n",
            "Epoch 01 - Acc: 0.9996, Prec: 0.0000, Rec: 0.0000, F1: 0.0000, AUC: 0.8834\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 2/5  (40.0% completed)\n",
            "Epoch 02 - Train Loss: 0.0036\n",
            "Evaluating on test set...\n",
            "Test Accuracy : 0.9996\n",
            "Test Precision: 0.0000\n",
            "Test Recall   : 0.0000\n",
            "Test F1       : 0.0000\n",
            "Test AUC      : 0.8903\n",
            "Epoch 02 - Acc: 0.9996, Prec: 0.0000, Rec: 0.0000, F1: 0.0000, AUC: 0.8903\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 3/5  (60.0% completed)\n",
            "Epoch 03 - Train Loss: 0.0031\n",
            "Evaluating on test set...\n",
            "Test Accuracy : 0.9997\n",
            "Test Precision: 1.0000\n",
            "Test Recall   : 0.1702\n",
            "Test F1       : 0.2909\n",
            "Test AUC      : 0.8895\n",
            "Epoch 03 - Acc: 0.9997, Prec: 1.0000, Rec: 0.1702, F1: 0.2909, AUC: 0.8895\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 4/5  (80.0% completed)\n",
            "Epoch 04 - Train Loss: 0.0030\n",
            "Evaluating on test set...\n",
            "Test Accuracy : 0.9997\n",
            "Test Precision: 1.0000\n",
            "Test Recall   : 0.1277\n",
            "Test F1       : 0.2264\n",
            "Test AUC      : 0.8994\n",
            "Epoch 04 - Acc: 0.9997, Prec: 1.0000, Rec: 0.1277, F1: 0.2264, AUC: 0.8994\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 5/5  (100.0% completed)\n",
            "Epoch 05 - Train Loss: 0.0028\n",
            "Evaluating on test set...\n",
            "Test Accuracy : 0.9997\n",
            "Test Precision: 1.0000\n",
            "Test Recall   : 0.3404\n",
            "Test F1       : 0.5079\n",
            "Test AUC      : 0.8836\n",
            "Epoch 05 - Acc: 0.9997, Prec: 1.0000, Rec: 0.3404, F1: 0.5079, AUC: 0.8836\n",
            "\n",
            "Training finished.\n",
            "Blockchain length (blocks): 7\n",
            "Last few blockchain blocks:\n",
            "{'index': 2, 'prev_hash': 'd84acf5722d667349d5fec7ea109afd4670b2b768937182861b0511a0d0e6349', 'data': {'tx_info': {'sender_node': 6724, 'receiver_node': 12175, 'true_label': 1}, 'risk_score': 0.9631097316741943}, 'hash': 'c0284a7ec5db8b3ded291f40142067a7e2d77d78e0cfb6a6d323c870db2bd660'}\n",
            "{'index': 3, 'prev_hash': 'c0284a7ec5db8b3ded291f40142067a7e2d77d78e0cfb6a6d323c870db2bd660', 'data': {'tx_info': {'sender_node': 3806, 'receiver_node': 34455, 'true_label': 1}, 'risk_score': 0.9746283888816833}, 'hash': '435facb50a9ffd244ed00118d0dd36201c17a39e73b54ee836a1c9aa1f446972'}\n",
            "{'index': 4, 'prev_hash': '435facb50a9ffd244ed00118d0dd36201c17a39e73b54ee836a1c9aa1f446972', 'data': {'tx_info': {'sender_node': 3806, 'receiver_node': 34455, 'true_label': 1}, 'risk_score': 0.9822198152542114}, 'hash': 'ffd939b11261562efbc19e6df15594679eccef29483ebf021a043c3d56909940'}\n",
            "{'index': 5, 'prev_hash': 'ffd939b11261562efbc19e6df15594679eccef29483ebf021a043c3d56909940', 'data': {'tx_info': {'sender_node': 7800, 'receiver_node': 15132, 'true_label': 1}, 'risk_score': 0.9590774178504944}, 'hash': 'd6d0bac2813fbddeb724d6b0ae6a481ee0e03aefb6b53a97bf27d89732e428d2'}\n",
            "{'index': 6, 'prev_hash': 'd6d0bac2813fbddeb724d6b0ae6a481ee0e03aefb6b53a97bf27d89732e428d2', 'data': {'tx_info': {'sender_node': 16036, 'receiver_node': 20504, 'true_label': 1}, 'risk_score': 0.9698588848114014}, 'hash': 'f4a7100362c0e8f08f99d3b04f60c03906af9273cc450a36546fcff686f13406'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    # Show which epoch and overall percentage\n",
        "    pct_done = 100.0 * epoch / EPOCHS\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}  ({pct_done:.1f}% completed)\")\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_one_epoch(epoch)\n",
        "\n",
        "    # Evaluate after every epoch\n",
        "    print(\"Evaluating on test set...\")\n",
        "    acc, prec, rec, f1, auc = evaluate(\n",
        "        blockchain_obj=blockchain,\n",
        "        alert_threshold=0.9\n",
        "    )\n",
        "\n",
        "    #Short summary line for that epoch\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} - \"\n",
        "        f\"Acc: {acc:.4f}, Prec: {prec:.4f}, \"\n",
        "        f\"Rec: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\"\n",
        "    )\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "print(\"Blockchain length (blocks):\", len(blockchain))\n",
        "print(\"Last few blockchain blocks:\")\n",
        "for block in blockchain.tail(5):\n",
        "    print(block)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Iq_dWYQbC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27038012-8eac-43d4-a77c-a12c3a771e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/drive/MyDrive/Research Folder (Al-Amin Dhaly)/hybrid_aml_saml_d.pt\n"
          ]
        }
      ],
      "source": [
        "#Save trained model to Drive\n",
        "save_path = \"/content/drive/MyDrive/Research Folder (Al-Amin Dhaly)/hybrid_aml_saml_d.pt\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Model saved to:\", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\"\"\""
      ],
      "metadata": {
        "id": "r48wfi_aCet2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}