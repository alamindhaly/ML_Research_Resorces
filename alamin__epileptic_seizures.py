# -*- coding: utf-8 -*-
"""Alamin_ Epileptic_Seizures

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xk2Y0zduW5zFSSLrrWfR0YNQT2QrxkXZ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from google.colab import files
uploaded= files.upload()
df=pd.read_csv('ep.csv')
df.head(10)

pip install vaex

df.info()

df.shape

df.isna().values.any()

df['y'].value_counts()

pip install matplotlib==3.1.1



#sns.countplot(df['y'],label='value')
sns.countplot(df.y,palette=["#6A1B4D","#35BDD0","#50DBB4","#6A1B4D","#50DBB4"])
#plt.title("[1]----> Secondery Stage  [0]----> Primary Stage"):
plt.ylabel('Combination of five stages values')
plt.xlabel('Predicted_attriute')

df.dtypes

deleted_column = df.pop(df.columns[0])

df.head(10)

"""#MinMax Scaler"""

from sklearn.preprocessing import MinMaxScaler

# # scale features
 scaler = MinMaxScaler()
 model=scaler.fit(df)
 scaled_data=model.transform(df)
 
# # print scaled features
 print(scaled_data)

from sklearn.preprocessing import MinMaxScaler

MMS = MinMaxScaler(feature_range=(0 , 1))

afterScaler = MMS.fit_transform(df)

afterScaler

"""#Correlation"""

#using Pearson Correlation
plt.figure(figsize=(20,20))
cor=df.corr()
sns.heatmap(cor,annot=True,cmap=plt.cm.Greys_r)
plt.show()

plt.figure(figsize=(15,15))
matrix = np.triu(df.corr())
sns.heatmap(df.corr(), annot=True, mask=matrix,cmap=plt.cm.PuBuGn)

"""**Train Test Soplit**"""

x=df.iloc[:,:-1].values
y=df.iloc[:,-1].values
from sklearn.model_selection import train_test_split 
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.20,random_state=42)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)

"""#File Import"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_digits
digits= load_digits()

"""**MODELS**"""

def models(x_train,y_train):

  #Random Forest         0
  forest=RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=30)
  forest.fit(x_train,y_train)

                 
  #Gradient Boosting     1   
  gb_clf=GradientBoostingClassifier ( n_estimators=30,max_features=1,random_state=42)
  gb_clf.fit(x_train,y_train)


  #k-Nearest Neighbors  2
  classifier = KNeighborsClassifier(n_neighbors=8)
  classifier.fit(x_train,y_train)

                   
  #XGBclassifier       3
  xgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,
                     max_depth = 5, alpha = 10, n_estimators = 10)
  xgb.fit(x_train, y_train)


  
  print('[0]Random Forest Training Accuracy:',forest.score(x_train,y_train))
  
  print('[1]Gradient Boosting Training Accuracy:',gb_clf.score(x_train,y_train))
  
  print('[2]k-Nearest Neighbors Training Accuracy:',classifier.score(x_train,y_train))
  
  print('[3]XGBclassifier Training Accuracy:',xgb.score(x_train,y_train))

  return forest,gb_clf,classifier,xgb

model = models(x_train, y_train)                       #model fitting

"""**Confusion Matrix**"""

from sklearn.metrics import confusion_matrix,classification_report,log_loss,cohen_kappa_score
from sklearn import metrics
for i in range (len(model)):
  print('confusion matrix of model',i,'is:')
  cm=confusion_matrix(y_test,model[i].predict(x_test))
  TP=cm[0][0]
  TN=cm[1][1]
  FP=cm[0][1]
  FN=cm[1][0]
  print(cm)
  print()
  result1=classification_report(y_test,model[i].predict(x_test))
  print("Classification report:",)
  print(result1)
  print()
  var=((TP+TN)/(TP+TN+FP+FN))*100
  print('testing accuracy:',var)
  print('Sensitivity:',TP/(TP+FN))
  print('Specificity:',TN/(TN+FP))
  print('false positive rate:',FP/(FP+TN))
  print('False negative:',FN/(FN+TP))
  print('Negative Peridictive Value:',TN/(TN+FN))
  print('False Discovery rate:',FP/(TP+FP))
  print('Mean Absolute Eror:',metrics.mean_absolute_error(y_test,model[i].predict(x_test)))
  print('Mean Squared Error:',metrics.mean_squared_error(y_test,model[i].predict(x_test)))
  print('Root Mean Squared Error:',np.sqrt(metrics.mean_squared_error(y_test,model[i].predict(x_test))))
  #print('log_loss:',metrics.log_loss(y_test,model[i].predict(x_test)))
  print('Çohen_Kappa_Scorer:',cohen_kappa_score(y_test,model[i].predict(x_test)))
  
  print()
  print()
  name=['RandomForestClassifier','GradientBoostingClassifier','KNeighborsClassifier','XGBClassifier']
  col_value=['blue','green','purple','blue']
  model_accuracy=pd.Series(data=(var),index=[name[i]])
  fig=plt.figure(figsize=(5,5))
  width=0.75
  model_accuracy.sort_values().plot.bar(alpha=0.8,color=[col_value[i]])
  plt.xticks(rotation=0)
  plt.title('model Accuracy')
  plt.ylabel('Accuracy(%)')
  plt.show()
  print()
  print()

"""#Ensemble Methods

#VOTING CLASSIFIER
"""

from sklearn.ensemble import VotingClassifier

G = GradientBoostingClassifier ( n_estimators=30,max_features=1,random_state=42)
K= KNeighborsClassifier(n_neighbors=8)
R = RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=30)
X = XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)
evc = VotingClassifier(estimators=[('G',G),('K',K),('R',R),('X',X)],voting='soft' )
evc.fit(x_train,y_train)
evc.score(x_test,y_test)

G = GradientBoostingClassifier ( n_estimators=30,max_features=1,random_state=42)
R = RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=30)
X = XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)
evc = VotingClassifier(estimators=[('G',G),('R',R),('X',X)],voting='soft' )
evc.fit(x_train,y_train)
evc.score(x_test,y_test)

K= KNeighborsClassifier(n_neighbors=8)
G= GradientBoostingClassifier ( n_estimators=30,max_features=1,random_state=42)
X=XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)
evc = VotingClassifier(estimators=[('K',K),('X',X),('G',G)],voting='soft')
evc.fit(x_train,y_train)
evc.score(x_test,y_test)

R= RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=30)
X= XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)
K=KNeighborsClassifier(n_neighbors=8)
evc = VotingClassifier(estimators=[('R',R),('X',X),('K',K)],voting='soft')
evc.fit(x_train,y_train)
evc.score(x_test,y_test)

R= RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=42)
G= GradientBoostingClassifier ( n_estimators=30,max_features=1,random_state=42)
K=KNeighborsClassifier(n_neighbors=8)
evc = VotingClassifier(estimators=[('R',R),('G',G),('K',K)],voting='soft' )
evc.fit(x_train,y_train)
evc.score(x_test,y_test)

K=KNeighborsClassifier(n_neighbors=8)
R= RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=42)
evc = VotingClassifier(estimators=[('K',K),('R',R)],voting='soft' )
evc.fit(x_train,y_train)
evc.score(x_test,y_test)

G=GradientBoostingClassifier( n_estimators=30,max_features=1,random_state=42)
X= XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 1,max_depth = 5, alpha = 10, n_estimators = 35)
evc = VotingClassifier(estimators=[('G',G),('X',X)],voting='soft' )
evc.fit(x_train,y_train)
evc.score(x_test,y_test)

"""**Bagging**"""

from sklearn.ensemble import BaggingClassifier
bg=BaggingClassifier(RandomForestClassifier(n_estimators=5,criterion='entropy',random_state=30),max_samples=0.5,
                     max_features=7,n_estimators=20)

bg.fit(x_train,y_train)
bg.score(x_test,y_test)

bg1 = BaggingClassifier(GradientBoostingClassifier ( n_estimators=30,loss= 'deviance', max_features=1,learning_rate= 0.2,random_state=60,
                                                    criterion='mse',warm_start= 'bool'))
bg1.fit(x_train,y_train)
bg1.score(x_test,y_test)

bg2 = BaggingClassifier(XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 30))
bg2.fit(x_train,y_train)
bg2.score(x_test,y_test)

bg3 = BaggingClassifier(KNeighborsClassifier(n_neighbors=15))
bg3.fit(x_train,y_train)
bg1.score(x_test,y_test)



"""#BOOSTING"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import AdaBoostClassifier
# bos=AdaBoostClassifier(GradientBoostingClassifier(n_estimators=30,max_features=1,random_state=42))
# bos.fit(x_train,y_train)

bos.score(x_test,y_test)

ypred9 = bos.predict(x_test)
pr, rc, fs, sup = metrics.precision_recall_fscore_support(y_test, ypred9, average='macro')
print("precision :", pr)
print("Recall :", rc)
print("F1 Score :", fs)

"""#Stacking"""

!apt-get install

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier

!pip install git+https://github.com/hyperopt/hyperopt-sklearn.git

clf1=GradientBoostingClassifier ( n_estimators=20,loss= 'deviance', max_features=1,learning_rate= 0.50,random_state=40,criterion='mse',verbose= 2,warm_start= 'bool')
clf2=KNeighborsClassifier(n_neighbors=8)
clf3=RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=30)
clf4=XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,
                     max_depth = 5, alpha = 10, n_estimators = 10)
log=LogisticRegression(random_state=42 )
sclf=StackingClassifier(classifiers=[clf1,clf2,clf3,clf4],use_probas=True,meta_classifier=log)

"""#AUC & ROC CURVE

**Random Forest**
"""

# multi-class classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# generate 2 class dataset
X, y = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=3, random_state=42)

# split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# fit model
clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=5,random_state = 10))
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
pred_prob = clf.predict_proba(X_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 3

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)
# plotting    
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('Multiclass ROC',dpi=300);

"""**KNeighborsClassifier**"""

# multi-class classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# generate 2 class dataset
X, y = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=3, random_state=42)

# split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# fit model
clf = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=8))
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
pred_prob = clf.predict_proba(X_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 3

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)
# plotting    
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('Multiclass ROC',dpi=300);

"""**Gradient Boosting**"""

# multi-class classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# generate 2 class dataset
X, y = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=3, random_state=42)

# split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# fit model
clf = OneVsRestClassifier(GradientBoostingClassifier( n_estimators=30,max_features=1,random_state=42))
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
pred_prob = clf.predict_proba(X_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 3

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)
# plotting    
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('Multiclass ROC',dpi=300);

"""**XGB Classifier**"""

# multi-class classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.datasets import make_classification
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# generate 2 class dataset
X, y = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=3, random_state=42)

# split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# fit model
clf = OneVsRestClassifier(XGBClassifier(colsample_bytree = 0.3, learning_rate = 0.1,
                     max_depth = 5, alpha = 10, n_estimators = 10))
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
pred_prob = clf.predict_proba(X_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 3

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)
# plotting    
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('Multiclass ROC',dpi=300);

"""#KFold cross validation"""

from sklearn.model_selection import KFold
kf = KFold(n_splits=5)
kf

for train_index,test_index in kf.split ([1,2,3,4,5,6,7]):
   print(train_index, test_index)

def get_score (model, x_train, x_test, y_train, y_test):
  model.fit(x_train, y_train)
  return model.score(x_test, y_test)

get_score(GradientBoostingClassifier(), x_train, x_test, y_train, y_test)

get_score(KNeighborsClassifier(), x_train, x_test, y_train, y_test)

get_score(RandomForestClassifier(), x_train, x_test, y_train, y_test)

get_score(XGBClassifier(), x_train, x_test, y_train, y_test)

"""**StratifiedKFold**"""

from sklearn.model_selection import StratifiedKFold
folds = StratifiedKFold (n_splits=3)

scores_gradient= []
scores_forest= []
scores_xgb = []
scores_classifier = []


for train_index, test_index in folds.split(digits.data,digits.target):
    x_train, x_test, y_train, y_test = digits.data[train_index], digits.data[test_index], \
                                       digits.target[train_index], digits.target[test_index]

scores_xgb.append(get_score(XGBClassifier() , x_train, x_test, y_train, y_test)) 
scores_classifier.append(get_score(KNeighborsClassifier(n_neighbors=8), x_train, x_test, y_train, y_test)) 
scores_forest.append(get_score(RandomForestClassifier(n_estimators=30), x_train, x_test, y_train, y_test))
scores_gradient.append(get_score(GradientBoostingClassifier(), x_train, x_test, y_train, y_test))

scores_forest

scores_xgb

scores_classifier           #KNeighborsClassifier

scores_gradient

"""**cross_val_score function**"""

from sklearn.model_selection import cross_val_score

cross_val_score(GradientBoostingClassifier(), digits.data, digits.target,cv=3)

cross_val_score(RandomForestClassifier(n_estimators=30),digits.data, digits.target,cv=3)

cross_val_score(KNeighborsClassifier(n_neighbors=8), digits.data, digits.target,cv=3)

cross_val_score(XGBClassifier(n_estimators=50), digits.data, digits.target,cv=3)

"""#Hyper Parameter Tuning

**Random Forest**
"""

param_grid = {  'bootstrap': [True],
              
              'max_features': ['auto', 'log2'], 
              'criterion' : ['entropy','gini',],
              'n_estimators': [1,2,4,6,8,5,10,15]       }

rfc=RandomForestClassifier(random_state=30)

cv1 = GridSearchCV(estimator = rfc, param_grid = param_grid,cv =3, n_jobs = 1, verbose = 0, return_train_score=True)

cv1.fit(x_train, y_train);
print(cv1.best_params_)

print(cv1.score(x_test, y_test))

cv1.best_score_

"""**Gradient Boosting**"""

gbc = GradientBoostingClassifier()
parameters = {
    "n_estimators":[30,40,50,60],
    "max_features":[1,2,5,9],
    "learning_rate":[0.1,1.5,1.75]
    
}

from sklearn.model_selection import GridSearchCV
cv2 = GridSearchCV(gbc,parameters,cv=5)

cv2.fit(x_train, y_train)

print(cv2.best_params_)

print(cv2.score(x_test, y_test))

cv2.best_score_

"""**K-Nearest Neighbors**"""

parameters2 = { 'n_neighbors' : [5,7,9,11,13,15],
               'weights' : ['uniform','distance'],
               'metric' : ['minkowski','euclidean','manhattan']}

cv3 = GridSearchCV(KNeighborsClassifier(), parameters2, verbose = 1, cv=3, n_jobs = -1)

cv3.fit(x_train, y_train)

print(cv3.best_params_)

print(cv3.score(x_test, y_test))

cv3.best_score_

"""**XGB Classifier**"""

parameters3 = { 'booster' : ['gbtree','gblinear'],
                'max_depth' : [1,2,3,4,5,7],
               'n_estimators' : [5,10,20,30,50],
               "learning_rate":[0.1,1.5,1.75]
    
}

cv4=GridSearchCV(XGBClassifier(), parameters3, verbose = 1, cv=3)

cv4.fit(x_train, y_train)

print(cv4.best_params_)

print(cv4.score(x_test, y_test))

cv4.best_score_

"""#Univariate Feature Selection

"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.model_selection import train_test_split

x=df.iloc[:,:-1]
y=df.iloc[:,-1]

bestfeatures = SelectKBest(score_func=f_classif)

fit = bestfeatures.fit(x, y)

dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(x.columns)

#concat two dataframes for better visualization
featurescore = pd.concat([dfcolumns,dfscores],axis=1)
featurescore.columns = ['Specs','Score']

featurescore

#print 100 best feature
feature = featurescore.nlargest(100,'Score')

feature

feature.sort_index()

featurescore.nlargest(90,'Score').plot(kind='bar')

"""//COLUMN DROP"""

df_new= df.drop(['X3','X4','X5','X16','X17','X18','X19','X20','X21','X22','X23','X37','X38','X39','X40','X47','X48','X49','X50','X51','X52','X53','X54','X58','X63','X64','X65','X74','X75','X76','X77','X78','X79','X80',
              'X86','X87','X88','X89','X90','X91','X100','X101','X105','X106','X107','X108','X109','X110','X111','X112','X113','X114','X118','X119','X120','X121','X122','X123','X127','X128','X129','X130','X131','X132','X133',
              'X134','X144','X145','X146','X163','X164','X165','X166','X167','X168','X169','X170','X171'],axis='columns')

df_new

df_new.corr()

mask = np.triu(np.ones_like(df_new.corr()))

plt.figure(figsize=(18,14))
sns.heatmap(df_new.corr(), mask=mask, vmax=.3,linewidths=1, square=True)
plt.show()

"""**#Working Again With Selected Features**

#Train Test Split2
"""

x1_train, x1_test, y1_train, y1_test =train_test_split(x1,y1,random_state=42 ,test_size=0.2)
print('test_X',x1_test.shape)
print('training_X',x1_train.shape)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x1_train = sc.fit_transform(x1_train)
x1_test = sc.fit_transform(x1_test)

"""#File Import2"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_digits
digits= load_digits()

def models(x1_train,y1_train):

  #Random Forest         0
  forest1=RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=30)
  forest1.fit(x1_train,y1_train)

                 
  #Gradient Boosting     1   
  gb_clf1=GradientBoostingClassifier ( n_estimators=30,max_features=1,random_state=42)
  gb_clf1.fit(x1_train,y1_train)


  #k-Nearest Neighbors  2
  classifier1 = KNeighborsClassifier(n_neighbors=8)
  classifier1.fit(x1_train,y1_train)

                   
  #XGBclassifier       3
  xgb1 =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,
                     max_depth = 5, alpha = 10, n_estimators = 10)
  xgb1.fit(x1_train, y1_train)


  
  print('[0]Random Forest Training Accuracy:',forest1.score(x1_train,y1_train))
  
  print('[1]Gradient Boosting Training Accuracy:',gb_clf1.score(x1_train,y1_train))
  
  print('[2]k-Nearest Neighbors Training Accuracy:',classifier1.score(x1_train,y1_train))
  
  print('[3]XGBclassifier Training Accuracy:',xgb1.score(x1_train,y1_train))

  return forest1,gb_clf1,classifier1,xgb1

model = models(x1_train, y1_train)

from sklearn.metrics import confusion_matrix,classification_report,log_loss,cohen_kappa_score
from sklearn import metrics
for i in range (len(model)):
  print('confusion matrix of model',i,'is:')
  cm=confusion_matrix(y1_test,model[i].predict(x1_test))
  TP=cm[0][0]
  TN=cm[1][1]
  FP=cm[0][1]
  FN=cm[1][0]
  print(cm)
  print()
  result1=classification_report(y1_test,model[i].predict(x1_test))
  print("Classification report:",)
  print(result1)
  print()
  var=((TP+TN)/(TP+TN+FP+FN))*100
  print('testing accuracy:',var)
  print('Sensitivity:',TP/(TP+FN))
  print('Specificity:',TN/(TN+FP))
  print('false positive rate:',FP/(FP+TN))
  print('False negative:',FN/(FN+TP))
  print('Negative Peridictive Value:',TN/(TN+FN))
  print('False Discovery rate:',FP/(TP+FP))
  print('Mean Absolute Eror:',metrics.mean_absolute_error(y1_test,model[i].predict(x1_test)))
  print('Mean Squared Error:',metrics.mean_squared_error(y1_test,model[i].predict(x1_test)))
  print('Root Mean Squared Error:',np.sqrt(metrics.mean_squared_error(y1_test,model[i].predict(x1_test))))
  #print('log_loss:',metrics.log_loss(y1_test,model[i].predict(x1_test)))
  print('Çohen_Kappa_Scorer:',cohen_kappa_score(y1_test,model[i].predict(x1_test)))
  
  print()
  print()
  name=['RandomForestClassifier','GradientBoostingClassifier','KNeighborsClassifier','XGBClassifier']
  col_value=['blue','green','purple','blue']
  model_accuracy=pd.Series(data=(var),index=[name[i]])
  fig=plt.figure(figsize=(5,5))
  width=0.75
  model_accuracy.sort_values().plot.bar(alpha=0.8,color=[col_value[i]])
  plt.xticks(rotation=0)
  plt.title('model Accuracy')
  plt.ylabel('Accuracy(%)')
  plt.show()
  print()
  print()

"""#Ensemble Methods

#VOTING CLASSIFIER 2
"""

from sklearn.ensemble import VotingClassifier

G1 = GradientBoostingClassifier ( n_estimators=30,max_features=1,random_state=42)
R1 = RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=30)
X1= XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)
evc = VotingClassifier(estimators=[('G1',G1),('R1',R1),('X1',X1)],voting='soft' )
evc.fit(x1_train,y1_train)
evc.score(x1_test,y1_test)

G1=GradientBoostingClassifier( n_estimators=30,max_features=1,random_state=42)
X1= XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 1,max_depth = 5, alpha = 10, n_estimators = 35)
K1=KNeighborsClassifier(n_neighbors=8)
R1= RandomForestClassifier(n_estimators=2,criterion='entropy',random_state=42)
evc = VotingClassifier(estimators=[('G1',G1),('X1',X1),('K1',K1),('R1',R1)],voting='soft')
evc.fit(x1_train,y1_train)
evc.score(x1_test,y1_test)

"""#Bagging 2"""

from sklearn.ensemble import BaggingClassifier
bg1=BaggingClassifier(RandomForestClassifier(n_estimators=5,criterion='entropy',random_state=30),max_samples=0.5,
                     max_features=7,n_estimators=20)

bg1.fit(x1_train,y1_train)
bg1.score(x1_test,y1_test)

bg2 = BaggingClassifier(GradientBoostingClassifier( n_estimators=30,loss= 'deviance', max_features=1,learning_rate= 0.2,random_state=60,
                                                    criterion='mse',warm_start= 'bool'))
bg2.fit(x1_train,y1_train)
bg2.score(x1_test,y1_test)

bg3 = BaggingClassifier(XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 30))
bg3.fit(x1_train,y1_train)
bg3.score(x1_test,y1_test)

bg4 = BaggingClassifier(KNeighborsClassifier(n_neighbors=15))
bg4.fit(x1_train,y1_train)
bg4.score(x1_test,y1_test)

"""#BOOSTING 2"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import AdaBoostClassifier
# bos1=AdaBoostClassifier(GradientBoostingClassifier(n_estimators=30,max_features=1,random_state=42))
# bos1.fit(x1_train,y1_train)

bos1.score(x1_test,y1_test)

ypred2 = bos1.predict(x1_test)
pr, rc, fs, sup = metrics.precision_recall_fscore_support(y1_test, ypred2, average='macro')
print("precision :", pr)
print("Recall :", rc)
print("F1 Score :", fs)

"""#AUC & ROC CURVE 2

**Random** **Forest**
"""

# multi-class classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# generate 2 class dataset
X1, y1 = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=3, random_state=42)

# split into train/test sets
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)

# fit model
clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=5,random_state = 10))
clf.fit(X1_train, y1_train)
pred = clf.predict(X1_test)
pred_prob = clf.predict_proba(X1_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 3

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y1_test, pred_prob[:,i], pos_label=i)
# plotting    
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('Multiclass ROC',dpi=300);

"""**KNeighborsClassifier**"""

# multi-class classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# generate 2 class dataset
X1, y1 = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=3, random_state=42)

# split into train/test sets
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)

# fit model
clf = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=8))
clf.fit(X1_train, y1_train)
pred = clf.predict(X1_test)
pred_prob = clf.predict_proba(X1_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 3

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y1_test, pred_prob[:,i], pos_label=i)
# plotting    
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('Multiclass ROC',dpi=300);

"""**Gradient Boosting**"""

# multi-class classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# generate 2 class dataset
X1, y1 = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=3, random_state=42)

# split into train/test sets
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)

# fit model
clf = OneVsRestClassifier(GradientBoostingClassifier( n_estimators=30,max_features=1,random_state=42))
clf.fit(X1_train, y1_train)
pred = clf.predict(X1_test)
pred_prob = clf.predict_proba(X1_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 3

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y1_test, pred_prob[:,i], pos_label=i)
# plotting    
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('Multiclass ROC',dpi=300);

"""**XGB Classifier**"""

# multi-class classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.datasets import make_classification
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# generate 2 class dataset
X1, y1 = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=3, random_state=42)

# split into train/test sets
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)

# fit model
clf = OneVsRestClassifier(XGBClassifier(colsample_bytree = 0.3, learning_rate = 0.1,
                     max_depth = 5, alpha = 10, n_estimators = 10))
clf.fit(X1_train, y1_train)
pred = clf.predict(X1_test)
pred_prob = clf.predict_proba(X1_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 3

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y1_test, pred_prob[:,i], pos_label=i)
# plotting    
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('Multiclass ROC',dpi=300);

"""#Hyper Parameter Tuning

**Random Forest**
"""

param_grid1 = {  'bootstrap': [True],
              
              'max_features': ['auto', 'log2'], 
              'criterion' : ['entropy','gini',],
              'n_estimators': [1,2,4,6,8,5,10,15]       }

rfc1=RandomForestClassifier(random_state=30)

cv2 = GridSearchCV(estimator = rfc1, param_grid = param_grid1,cv =3, n_jobs = 1, verbose = 0, return_train_score=True)

cv2.fit(x1_train, y1_train);
print(cv2.best_params_)